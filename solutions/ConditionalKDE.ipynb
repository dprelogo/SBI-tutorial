{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Gaussian KDE\n",
    "Here we go through a simple example of a conditional Gaussian KDE.\n",
    "Let's imagine we would like to sample from the distribution $P(x|y)$, given that samples from a full distribution $P(x, y)$ are available. Example of such scenario could be an MCMC chain, fitted for both $x$ and $y$.\n",
    "\n",
    "In general, such procedure is not possible as one would need to run MCMC from beginning, by sampling $P(x|y)$ directly, for one particular $y$. However, if we make an analytical form of the total pdf, then in principle we could obtain such conditional distribution for any $y$.\n",
    "\n",
    "`ConditionalGaussianKernelDensity` class implements these functionalities:\n",
    "\n",
    "- fitting the Gaussian KDE for the total distribution $P(x, y)$ form its samples,\n",
    "- slicing through it to either obtain samples of the $P(x | y)$, or calculate the exact values of the conditional probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from conditional_kde import ConditionalGaussianKernelDensity\n",
    "import ultranest\n",
    "import corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's create a simple 2D sample of points and fit KDE with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean1, cov1 = [0, -5], [[1, 0], [0, 5]]\n",
    "mean2, cov2 = [10, 5], [[5, 0], [0, 1]]\n",
    "data_xy = np.concatenate(\n",
    "    (\n",
    "        np.random.multivariate_normal(mean1, cov1, 10000), \n",
    "        np.random.multivariate_normal(mean2, cov2, 10000)\n",
    "    ), \n",
    "    axis = 0\n",
    ")\n",
    "data_y = data_xy[:, 1, np.newaxis]\n",
    "plt.figure(figsize = (5, 5))\n",
    "plt.hist2d(data_xy[:, 0], data_xy[:, 1], bins = 50)\n",
    "plt.xlabel(\"$x$\", fontsize = 20)\n",
    "plt.ylabel(\"$y$\", fontsize = 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two most important parameters of the KDE are `whitening_algorithm` and `bandwidth`.\n",
    "Whitening is the process of normalizing the data in some way, here we implement three different ones:\n",
    "- `None` - no normalization\n",
    "- `\"rescale\"` - rescaling every dimension to be of unit-variance\n",
    "- `\"ZCA\"` - whitening along principal component axes\n",
    "\n",
    "Bandwidth on the other hand controls the size of Gaussians around each point. Higher bandwidth will give wider and smoother distributions. We implement several ways of choosing its value:\n",
    "- `\"scott\"` - uses approximate relation depending on the number of samples and dimensions - Scott's parameter\n",
    "- `\"optimized\"` - searches for the best bandwidth by minimizing KL divergence between real distribution samples and fitted KDE\n",
    "\n",
    "Besides that, we allow to specify its value explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kde = ConditionalGaussianKernelDensity(\n",
    "    whitening_algorithm = \"rescale\",\n",
    "    bandwidth = \"optimized\",\n",
    "    steps = 10,\n",
    "    cv_fold = 5,\n",
    "    n_jobs = -1,\n",
    "    verbose = 1,\n",
    ")\n",
    "kde = kde.fit(\n",
    "    data_xy,\n",
    "    features = [\"x\", \"y\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can either pull samples for some conditional values of y, or calculate analytical distributions for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mini, maxi = -5, 15\n",
    "X = np.linspace(mini, maxi, 100) \n",
    "fig, axes = plt.subplots(1, 3, sharey = True, figsize = (15, 5))\n",
    "epsilon = 2.0\n",
    "for y, ax in zip([-5, 1.5, 5], axes):\n",
    "    # plotting histogram of actual samples\n",
    "    ax.hist(\n",
    "        data_xy[\n",
    "            np.logical_and(\n",
    "                data_xy[:, 1] > y - epsilon / 2, \n",
    "                data_xy[:, 1] < y + epsilon / 2,\n",
    "            ), \n",
    "            0\n",
    "        ], \n",
    "        # bins = 100, \n",
    "        density = True, \n",
    "        label = \"samples\", \n",
    "        histtype = \"step\",\n",
    "        color = \"red\",\n",
    "    )\n",
    "    # pulling conditional samples\n",
    "    data_x_cond_y = kde.sample(\n",
    "        conditionals = {\"y\": y},\n",
    "        n_samples = 10000,\n",
    "        keep_dims = False,\n",
    "    )\n",
    "    # plotting histogram\n",
    "    ax.hist(\n",
    "        data_x_cond_y, \n",
    "        bins = 100, \n",
    "        density = True, \n",
    "        label = \"KDE samples\", \n",
    "        histtype = \"step\",\n",
    "        color = \"blue\",\n",
    "    )\n",
    "    \n",
    "    # calculating analytical probabilities along the axis\n",
    "    x = np.stack([X, np.ones(len(X)) * y], axis = -1)\n",
    "    probs = np.exp(kde.score_samples(x, conditional_features = [\"y\"]))\n",
    "    # plotting the function\n",
    "    ax.plot(X, probs, label = \"KDE analytic\", color = \"black\")\n",
    "\n",
    "    ax.set_title(f\"$P(x | y = {y})$\", fontsize = 20)\n",
    "    ax.set_xlim((mini, maxi))\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([-5, 0, 5, 10, 15])\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZCA whitening\n",
    "Instead of a simple rescale of dimensions, potentially more powerful whitening can be obtained with ZCA algorithm. It scales the data along the principal axis. The method comes with a bit bigger computational overhead. Here we show results for the same example as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kde = ConditionalGaussianKernelDensity(\n",
    "    whitening_algorithm = \"ZCA\", \n",
    "    bandwidth = \"optimized\",\n",
    "    steps = 10,\n",
    "    cv_fold = 5,\n",
    "    n_jobs = -1,\n",
    "    verbose = 1,\n",
    ")\n",
    "kde = kde.fit(\n",
    "    data_xy, \n",
    "    features = [\"x\", \"y\"], \n",
    ")\n",
    "\n",
    "mini, maxi = -5, 15\n",
    "X = np.linspace(mini, maxi, 100)\n",
    "fig, axes = plt.subplots(1, 3, sharey = True, figsize = (15, 5))\n",
    "epsilon = 2.0\n",
    "for y, ax in zip([-5, 1.5, 5], axes):\n",
    "    # plotting histogram of actual samples\n",
    "    ax.hist(\n",
    "        data_xy[\n",
    "            np.logical_and(\n",
    "                data_xy[:, 1] > y - epsilon / 2, \n",
    "                data_xy[:, 1] < y + epsilon / 2,\n",
    "            ), \n",
    "            0\n",
    "        ], \n",
    "        # bins = 100, \n",
    "        density = True, \n",
    "        label = \"samples\", \n",
    "        histtype = \"step\",\n",
    "        color = \"red\",\n",
    "    )\n",
    "    # pulling conditional samples\n",
    "    data_x_cond_y = kde.sample(\n",
    "        conditionals = {\"y\": y},\n",
    "        n_samples = 10000,\n",
    "        keep_dims = False,\n",
    "    )\n",
    "    # plotting histogram\n",
    "    ax.hist(\n",
    "        data_x_cond_y, \n",
    "        bins = 100, \n",
    "        density = True, \n",
    "        label = \"KDE samples\", \n",
    "        histtype = \"step\",\n",
    "        color = \"blue\",\n",
    "    )\n",
    "    \n",
    "    # calculating analytical probabilities along the axis\n",
    "    x = np.stack([X, np.ones(len(X)) * y], axis = -1)\n",
    "    probs = np.exp(kde.score_samples(x, conditional_features = [\"y\"]))\n",
    "    # plotting the function\n",
    "    ax.plot(X, probs, label = \"KDE analytic\", alpha = 0.7, color = \"black\")\n",
    "\n",
    "    ax.set_title(f\"$P(x | y = {y})$\", fontsize = 20)\n",
    "    ax.set_xlim((mini, maxi))\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([-5, 0, 5, 10, 15])\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK:\n",
    "Pretend that variable $y$ represents the parameter of your model and $x$ is the data of your model.\n",
    "Using the fitted conditional distribution $P(x | y)$, and additional prior $P(y) = \\mathcal{N}(y | \\mu = 0, \\sigma^2 = 2)$, recover the posterior $P(y | x = 1.0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior(y):\n",
    "    # write your prior distribution\n",
    "    # y is received as (N, 1) array\n",
    "    # log-probability has to be returned as (N,) array\n",
    "    l = -y**2 / 4\n",
    "    return l.flatten()\n",
    "\n",
    "def log_likelihood(y, x = 1.0):\n",
    "    # write your likelihood\n",
    "    # y is received as (N, 1) array\n",
    "    # log-probability has to be returned as (N,) array\n",
    "    # hint: look at how KDE analytic was plotted in the previous example\n",
    "    N = len(y)\n",
    "    total = np.stack([np.ones(N) * x, y.flatten()], axis = -1)\n",
    "    log_probs = kde.score_samples(total, conditional_features = [\"y\"])\n",
    "    return log_probs.flatten()\n",
    "\n",
    "def total_distribution(y):\n",
    "    # write total distribution needed to be sampled\n",
    "    # y is received as (N, 1) array\n",
    "    # log-probability has to be returned as (N,) array\n",
    "    return log_prior(y) + log_likelihood(y)\n",
    "\n",
    "def transform(p):\n",
    "    # transforming [0, 1] samples into sensible ranges of y [-5, 5]\n",
    "    y = -5 + 10 * p\n",
    "    return p * 10 - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampler = ultranest.ReactiveNestedSampler(\n",
    "    [\"y\"], \n",
    "    loglike = total_distribution, \n",
    "    transform = transform,\n",
    "    vectorized = True,\n",
    "    draw_multiple = True,\n",
    ")\n",
    "result = sampler.run(min_num_live_points = 1000)\n",
    "sampler.print_results()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "58ae7b3716633041a01a40c36e0d7b7460b562af78f6efaf769f0d9c360a3f13"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
